[train] #train parameters
optimizer = adam
learning_rate = 1E-5

[data] #data parameters
data_path = ../datasets/WikiET
max_seq_length = 64
ke_path = ../knowledge_embedding/wikipedia/transe.npy

[model] #model parameters
PLM_path = bert-base-uncased

[output] #output parameters
model_path = ../mapping_networks/Downstream
model_name = WikiET

[mapper] #mapper parameters
mapper_type = Affine
mapper_path = None
input_dim = 128
output_dim = 768